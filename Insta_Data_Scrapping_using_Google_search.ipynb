{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Insta Data Scrapping using Google search.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVk9nVfXuRRE"
      },
      "source": [
        "**Test for Python Developer – API Data Extraction role**\n",
        "\n",
        "1. Uses google search to get insta profile ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWitN5wmuNEg"
      },
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import pandas as pd  \n",
        "import urllib\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqTDekfN_we4"
      },
      "source": [
        "**step 1 :** Get insta user links from google search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPrxxQmFzUjX"
      },
      "source": [
        "PROJECT_DIR = '/content/drive/MyDrive/DataExtraction'\n",
        "DATA_DIR = os.path.join(PROJECT_DIR,'ScrappedData')\n",
        "INSTA_DATA_DIR = os.path.join(DATA_DIR,'InstaData')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMoGqX7Wz52r"
      },
      "source": [
        "if not os.path.exists(PROJECT_DIR):\n",
        "  os.mkdir(PROJECT_DIR)\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "  os.mkdir(DATA_DIR)\n",
        "\n",
        "if not os.path.exists(INSTA_DATA_DIR):\n",
        "  os.mkdir(INSTA_DATA_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TBje98L2pMK"
      },
      "source": [
        "search_url = 'https://www.google.com/search?q=site%3Ainstagram.com+and+intitle%3A%40'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQWpkO5r_COz"
      },
      "source": [
        "# function to get insta profile urls from bs4 element\n",
        "def get_profile_url(heading_element):\n",
        "  link = heading_element.find_parent('a')\n",
        "  link_href = link.get('href')\n",
        "  profile_link = link_href.split('=')[1]\n",
        "  profile_link = profile_link.split('/&')[0]\n",
        "  return profile_link\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drV2bSpRu-tC",
        "outputId": "c0ce133c-8334-464f-803b-401634adbe86"
      },
      "source": [
        "insta_user_ids = []\n",
        "insta_user_urls = []\n",
        "\n",
        "search_items = 10\n",
        "#loop over search pages\n",
        "limit_pages = 100\n",
        "page_no = 1\n",
        "\n",
        " #loop for 100 google search pages\n",
        "for page in range(limit_pages):\n",
        "  #get the page\n",
        "  data = requests.get(search_url)\n",
        "  print(\"Scrapping data from page : {}\".format(page_no))\n",
        "  #create bs4 soup object\n",
        "  soup = BeautifulSoup(data.content, \"html.parser\")\n",
        "  #extract headings\n",
        "  headings = soup.find_all( 'h3' )\n",
        "  #loop over headings of curr page \n",
        "  for heading in headings:\n",
        "    if '@' in heading.getText():\n",
        "      profile_url = get_profile_url(heading)\n",
        "      insta_id = re.findall('\\(@.*?\\)', heading.getText())\n",
        "      if len(insta_id) > 0:\n",
        "        insta_id = insta_id[0].replace('(','').replace(')','')\n",
        "        insta_user_ids.append(insta_id)\n",
        "        insta_user_urls.append(profile_url)\n",
        "  \n",
        "  #update search url for new page\n",
        "  search_url = search_url + '&start='+str(search_items)\n",
        "  search_items = search_items + 10\n",
        "  page_no = page_no + 1\n",
        "  print(\"Insta URLS saved : {}\".format(len(insta_user_ids)))\n",
        "  time.sleep(3.6)\n",
        "\n",
        "#save as csv file\n",
        "\n",
        "dict = {'insta_id': insta_user_ids, 'profile_urls': insta_user_urls}  \n",
        "\n",
        "df = pd.DataFrame(dict) \n",
        "    \n",
        "# saving the dataframe \n",
        "df.to_csv(os.path.join(DATA_DIR,'insta_users_data1.csv')) \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scrapping data from page : 1\n",
            "Insta URLS saved : 7\n",
            "Scrapping data from page : 2\n",
            "Insta URLS saved : 16\n",
            "Scrapping data from page : 3\n",
            "Insta URLS saved : 22\n",
            "Scrapping data from page : 4\n",
            "Insta URLS saved : 29\n",
            "Scrapping data from page : 5\n",
            "Insta URLS saved : 38\n",
            "Scrapping data from page : 6\n",
            "Insta URLS saved : 47\n",
            "Scrapping data from page : 7\n",
            "Insta URLS saved : 56\n",
            "Scrapping data from page : 8\n",
            "Insta URLS saved : 64\n",
            "Scrapping data from page : 9\n",
            "Insta URLS saved : 72\n",
            "Scrapping data from page : 10\n",
            "Insta URLS saved : 81\n",
            "Scrapping data from page : 11\n",
            "Insta URLS saved : 86\n",
            "Scrapping data from page : 12\n",
            "Insta URLS saved : 94\n",
            "Scrapping data from page : 13\n",
            "Insta URLS saved : 104\n",
            "Scrapping data from page : 14\n",
            "Insta URLS saved : 113\n",
            "Scrapping data from page : 15\n",
            "Insta URLS saved : 122\n",
            "Scrapping data from page : 16\n",
            "Insta URLS saved : 130\n",
            "Scrapping data from page : 17\n",
            "Insta URLS saved : 137\n",
            "Scrapping data from page : 18\n",
            "Insta URLS saved : 144\n",
            "Scrapping data from page : 19\n",
            "Insta URLS saved : 153\n",
            "Scrapping data from page : 20\n",
            "Insta URLS saved : 161\n",
            "Scrapping data from page : 21\n",
            "Insta URLS saved : 168\n",
            "Scrapping data from page : 22\n",
            "Insta URLS saved : 175\n",
            "Scrapping data from page : 23\n",
            "Insta URLS saved : 182\n",
            "Scrapping data from page : 24\n",
            "Insta URLS saved : 192\n",
            "Scrapping data from page : 25\n",
            "Insta URLS saved : 202\n",
            "Scrapping data from page : 26\n",
            "Insta URLS saved : 210\n",
            "Scrapping data from page : 27\n",
            "Insta URLS saved : 219\n",
            "Scrapping data from page : 28\n",
            "Insta URLS saved : 227\n",
            "Scrapping data from page : 29\n",
            "Insta URLS saved : 234\n",
            "Scrapping data from page : 30\n",
            "Insta URLS saved : 244\n",
            "Scrapping data from page : 31\n",
            "Insta URLS saved : 248\n",
            "Scrapping data from page : 32\n",
            "Insta URLS saved : 251\n",
            "Scrapping data from page : 33\n",
            "Insta URLS saved : 251\n",
            "Scrapping data from page : 34\n",
            "Insta URLS saved : 251\n",
            "Scrapping data from page : 35\n",
            "Insta URLS saved : 251\n",
            "Scrapping data from page : 36\n",
            "Insta URLS saved : 252\n",
            "Scrapping data from page : 37\n",
            "Insta URLS saved : 254\n",
            "Scrapping data from page : 38\n",
            "Insta URLS saved : 257\n",
            "Scrapping data from page : 39\n",
            "Insta URLS saved : 263\n",
            "Scrapping data from page : 40\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 41\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 42\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 43\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 44\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 45\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 46\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 47\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 48\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 49\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 50\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 51\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 52\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 53\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 54\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 55\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 56\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 57\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 58\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 59\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 60\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 61\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 62\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 63\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 64\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 65\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 66\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 67\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 68\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 69\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 70\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 71\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 72\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 73\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 74\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 75\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 76\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 77\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 78\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 79\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 80\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 81\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 82\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 83\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 84\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 85\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 86\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 87\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 88\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 89\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 90\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 91\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 92\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 93\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 94\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 95\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 96\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 97\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 98\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 99\n",
            "Insta URLS saved : 268\n",
            "Scrapping data from page : 100\n",
            "Insta URLS saved : 268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1mWLvJx4PSf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cStsoQCO4P_g"
      },
      "source": [
        "**Step 2:** use obtained insta profile urls and get all required data using instascrape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "b4o_qbFncmUQ",
        "outputId": "9951d7b4-b877-4d30-d88f-a7189839117f"
      },
      "source": [
        "#load the csv file\n",
        "\n",
        "insta_users_info = pd.read_csv(os.path.join(DATA_DIR,'insta_users_data1.csv'))\n",
        "\n",
        "insta_users_info.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>insta_id</th>\n",
              "      <th>profile_urls</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@theoldmanandthethree</td>\n",
              "      <td>https://www.instagram.com/theoldmanandthethree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>@mtv</td>\n",
              "      <td>https://www.instagram.com/mtv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>@valkyrae</td>\n",
              "      <td>https://www.instagram.com/valkyrae</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>@lilyjcollins</td>\n",
              "      <td>https://www.instagram.com/lilyjcollins</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>@hm</td>\n",
              "      <td>https://www.instagram.com/hm</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                    profile_urls\n",
              "0           0  ...  https://www.instagram.com/theoldmanandthethree\n",
              "1           1  ...                   https://www.instagram.com/mtv\n",
              "2           2  ...              https://www.instagram.com/valkyrae\n",
              "3           3  ...          https://www.instagram.com/lilyjcollins\n",
              "4           4  ...                    https://www.instagram.com/hm\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8-TZBQEdmec"
      },
      "source": [
        "insta_ids = insta_users_info['insta_id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bffu0aMC8mkx",
        "outputId": "18933437-ce82-4d9d-f1a7-63a964358182"
      },
      "source": [
        "# install chromium, its driver, and selenium\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "# set options to be headless, ..\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "# open it, go to a website, and get results\n",
        "wd = webdriver.Chrome(options=options)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,428 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,326 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,799 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,202 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [921 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,761 kB]\n",
            "Fetched 11.7 MB in 3s (3,374 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "52 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 52 not upgraded.\n",
            "Need to get 91.8 MB of archives.\n",
            "After this operation, 315 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 92.0.4515.159-0ubuntu0.18.04.1 [1,124 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 92.0.4515.159-0ubuntu0.18.04.1 [81.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 92.0.4515.159-0ubuntu0.18.04.1 [4,026 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 92.0.4515.159-0ubuntu0.18.04.1 [4,902 kB]\n",
            "Fetched 91.8 MB in 5s (19.5 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155013 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_92.0.4515.159-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_92.0.4515.159-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_92.0.4515.159-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_92.0.4515.159-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting selenium\n",
            "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
            "\u001b[K     |████████████████████████████████| 904 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hurOyiUQLITh"
      },
      "source": [
        "headers = {\n",
        "    \"user-agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Mobile Safari/537.36 Edg/87.0.664.57\",\n",
        "    \"cookie\": \"sessionid=33726898013%3AiXbdXjOUcT2dHd%3A27;\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgQBQDHf09wf",
        "outputId": "5ade86f8-12c7-423d-b2eb-1f76c9e98dbd"
      },
      "source": [
        "!pip install insta-scrape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting insta-scrape\n",
            "  Downloading insta_scrape-2.1.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from insta-scrape) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from insta-scrape) (4.6.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->insta-scrape) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->insta-scrape) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->insta-scrape) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->insta-scrape) (2.10)\n",
            "Installing collected packages: insta-scrape\n",
            "Successfully installed insta-scrape-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sq0hCJn07VM"
      },
      "source": [
        "from instascrape import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgqOKN2t3KBT"
      },
      "source": [
        "def get_image(url,name):\n",
        "   urllib.request.urlretrieve(url, name)\n",
        "   i = Image.open(name)\n",
        "   i.thumbnail((100, 100), Image.LANCZOS)\n",
        "   i.save(name)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO_B5rNEFM2L"
      },
      "source": [
        "def get_twitter_acc(username):\n",
        "  search_query = 'https://www.google.com/search?q=site%3Atwitter.com+and+intitle:'+username\n",
        "  data = requests.get(search_query)\n",
        "  soup = BeautifulSoup(data.content, \"html.parser\")\n",
        "  #extract headings\n",
        "  headings = soup.find_all( 'h3' )\n",
        "  #loop over headings of curr page \n",
        "  twitter_link = ' '\n",
        "  twitter_id= ' '\n",
        "  for heading in headings:\n",
        "    if username in heading.getText().lower():\n",
        "      twitter_id = re.findall('\\(@.*?\\)', heading.getText())\n",
        "      if len(twitter_id) > 0:\n",
        "        twitter_id= twitter_id[0].replace('(','').replace(')','')\n",
        "        twitter_link = get_profile_url(heading).split('&')[0]\n",
        "        return twitter_link\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnCxoGHeJENL"
      },
      "source": [
        "def get_youtube_acc(username):\n",
        "  search_query = 'https://www.google.com/search?q='+username\n",
        "  data = requests.get(search_query)\n",
        "  soup = BeautifulSoup(data.content, \"html.parser\")\n",
        "  #extract headings\n",
        "  headings = soup.find_all( 'h3' )\n",
        "  #loop over headings of curr page \n",
        "  youtube_link = ' '\n",
        "  for heading in headings:\n",
        "    parent_el = str(heading.parent)\n",
        "    if 'youtube.com' in parent_el and 'watch' not in parent_el and username in parent_el:\n",
        "      youtube_link = parent_el.split('=')[2].split('&')[0]\n",
        "      return youtube_link"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUfsPMMjx84B",
        "outputId": "407eca53-7285-41d5-d933-f22aa5f6ac92"
      },
      "source": [
        "#xls\n",
        "!pip3 install xlsxwriter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.1-py3-none-any.whl (148 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 30 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 40 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 148 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxKQQLSjRIW1"
      },
      "source": [
        "import xlsxwriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Qb08l4QncL",
        "outputId": "fb36d76f-7970-4512-d3e4-add84bc0234b"
      },
      "source": [
        "for insta_id in insta_ids:\n",
        "  insta_id = insta_id.replace('@','')\n",
        "  print(\"Scrapping details from id :\"+str(insta_id))\n",
        "  user = Profile(insta_id)\n",
        "  user.scrape(headers=headers)\n",
        "  follower_count = user.followers\n",
        "  following_count = user.following\n",
        "  username = user.username\n",
        "  full_name = user.full_name\n",
        "  url = user.url\n",
        "  verfified = str(user.is_verified)\n",
        "  profile_pic_url = user.profile_pic_url\n",
        "  category = user.category_enum\n",
        "  num_posts = user.posts\n",
        "  biography = user.json_dict['entry_data']['ProfilePage'][0]['graphql']['user']['biography']\n",
        "  connected_fb_page = user.json_dict['entry_data']['ProfilePage'][0]['graphql']['user']['connected_fb_page']\n",
        "  web_page = user.json_dict['entry_data']['ProfilePage'][0]['graphql']['user']['external_url']\n",
        "  is_business_account = user.json_dict['entry_data']['ProfilePage'][0]['graphql']['user']['is_business_account']\n",
        "  twitter_acc_link = get_twitter_acc(full_name) \n",
        "  youtube_acc = get_youtube_acc(full_name)\n",
        "  if (is_business_account):\n",
        "    acc_type = 'Business'\n",
        "  else:\n",
        "    acc_type = ' '\n",
        "  get_image(profile_pic_url,'profile_pic.png')\n",
        "\n",
        "  workbook = xlsxwriter.Workbook(str(INSTA_DATA_DIR)+'/insta_'+str(insta_id)+'.xlsx')\n",
        "  worksheet = workbook.add_worksheet()\n",
        "  worksheet.write('A1', 'Name')\n",
        "  worksheet.write('B1', 'UserName')\n",
        "  worksheet.write('C1', 'Age')\n",
        "  worksheet.write('D1', 'Topics')\n",
        "  worksheet.write('E1', 'Followers')\n",
        "  worksheet.write('F1', 'Following')\n",
        "  worksheet.write('G1', 'AVG Likes')\n",
        "  worksheet.write('H1', 'Avg. Comments')\n",
        "  worksheet.write('I1', 'Eng Rate')\n",
        "  worksheet.write('J1', 'Total Posts')\n",
        "  worksheet.write('K1', 'Email')\n",
        "  worksheet.write('L1', 'Website')\n",
        "  worksheet.write('M1', 'Bio')\n",
        "  worksheet.write('N1', 'Gender')\n",
        "  worksheet.write('O1', 'Location')\n",
        "  worksheet.write('P1', 'Language')\n",
        "  worksheet.write('Q1', 'Youtube')\n",
        "  worksheet.write('R1', 'Twitter')\n",
        "  worksheet.write('S1', 'TikTok')\n",
        "  worksheet.write('T1', 'Facebook')\n",
        "  worksheet.write('U1', 'Profile Picture url')\n",
        "  worksheet.write('V1', 'Profile Picture')\n",
        "  worksheet.write('W1', 'Verified')\n",
        "  worksheet.write('X1', 'Account type')\n",
        "\n",
        "  worksheet.set_row(1,50)\n",
        "  worksheet.set_column_pixels(22,2,60)\n",
        "\n",
        "  worksheet.write('A2', full_name)\n",
        "  worksheet.write('B2', username)\n",
        "  worksheet.write('C2', 'NA')\n",
        "  worksheet.write('D2', 'NA')\n",
        "  worksheet.write('E2', follower_count)\n",
        "  worksheet.write('F2', following_count)\n",
        "  worksheet.write('G2', ' ')\n",
        "  worksheet.write('H2', ' ')\n",
        "  worksheet.write('I2', ' ')\n",
        "  worksheet.write('J2', num_posts)\n",
        "  worksheet.write('K2', ' ')\n",
        "  worksheet.write('L2', web_page)\n",
        "  worksheet.write('M2', biography)\n",
        "  worksheet.write('N2', ' ')\n",
        "  worksheet.write('O2', ' ')\n",
        "  worksheet.write('P2', ' ')\n",
        "  worksheet.write('Q2', youtube_acc)\n",
        "  worksheet.write('R2', twitter_acc_link)\n",
        "  worksheet.write('S2', ' ')\n",
        "  worksheet.write('T2', connected_fb_page)\n",
        "  worksheet.write('U2', profile_pic_url)\n",
        "  worksheet.insert_image('V2', 'profile_pic.png')\n",
        "  worksheet.write('W2', verfified)\n",
        "  worksheet.write('X2',acc_type )\n",
        "\n",
        "  worksheet.set_column('A:A', 30)\n",
        "\n",
        "  #save recent postsheading\n",
        "\n",
        "  worksheet.write('A11', 'Posts')\n",
        "  worksheet.write('B11', 'Description')\n",
        "  worksheet.write('C11', 'Hashtags')\n",
        "  worksheet.write('D11', 'Tags')\n",
        "  worksheet.write('E11', 'Likes')\n",
        "  worksheet.write('F11', 'Comments')\n",
        "  worksheet.write('G11', 'Image/Video URL')\n",
        "  worksheet.write('H11', 'Post URL')\n",
        "  recent_posts = user.get_recent_posts()\n",
        "  row = 12\n",
        "  for post in recent_posts:\n",
        "    post_url = post.url\n",
        "    post_media_url = post.display_url\n",
        "    get_image(post_media_url,str(row)+'.png')\n",
        "    try:\n",
        "      post_description = post.json_dict['edge_media_to_caption']['edges'][0]['node']['text']\n",
        "    except:\n",
        "      post_description = ' '\n",
        "    hashtags = regex = re.findall(\"#(\\w+)\", post_description)\n",
        "    hashtags = \"#\"+\",#\".join(hashtags)\n",
        "    num_likes = post.likes\n",
        "    num_comments = post.comments\n",
        "\n",
        "    worksheet.set_row(row-1,80)\n",
        "    worksheet.set_column_pixels(1,row,100)\n",
        "\n",
        "\n",
        "    worksheet.insert_image('A'+str(row), str(row)+'.png')\n",
        "    worksheet.write('B'+str(row), post_description)\n",
        "    worksheet.write('C'+str(row), hashtags)\n",
        "    worksheet.write('D'+str(row), ' ')\n",
        "    worksheet.write('E'+str(row), num_likes)\n",
        "    worksheet.write('F'+str(row), num_comments)\n",
        "    worksheet.write('G'+str(row), post_media_url)\n",
        "    worksheet.write('H'+str(row), post_url)\n",
        "    row = row +1\n",
        "\n",
        "  workbook.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scrapping details from id :theoldmanandthethree\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/instascrape/core/_static_scraper.py:136: MissingCookiesWarning: Request header does not contain cookies! It's recommended you pass at least a valid sessionid otherwise Instagram will likely redirect you to their login page.\n",
            "  MissingCookiesWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scrapping details from id :mtv\n",
            "Scrapping details from id :valkyrae\n",
            "Scrapping details from id :lilyjcollins\n",
            "Scrapping details from id :hm\n",
            "Scrapping details from id :kyliejenner\n",
            "Scrapping details from id :kendalljenner\n",
            "Scrapping details from id :kateupton\n",
            "Scrapping details from id :christojeanneclaude\n",
            "Scrapping details from id :lizgillz\n",
            "Scrapping details from id :justlikethatmax\n",
            "Scrapping details from id :rafaelloandco\n",
            "Scrapping details from id :channingtatum\n",
            "Scrapping details from id :haileesteinfeld\n",
            "Scrapping details from id :t22felton\n",
            "Scrapping details from id :keke\n",
            "Scrapping details from id :fatjoe\n",
            "Scrapping details from id :dazed\n",
            "Scrapping details from id :asabopp\n",
            "Scrapping details from id :haileybieber\n",
            "Scrapping details from id :kellyclarkson\n",
            "Scrapping details from id :danandshay\n",
            "Scrapping details from id :nikitadragun\n",
            "Scrapping details from id :hoskelsa\n",
            "Scrapping details from id :rickandmorty\n",
            "Scrapping details from id :oliviawilde\n",
            "Scrapping details from id :lilireinhart\n",
            "Scrapping details from id :fazerug\n",
            "Scrapping details from id :ellenmint_\n",
            "Scrapping details from id :shangchi\n",
            "Scrapping details from id :foodandwine\n",
            "Scrapping details from id :iamhalsey\n",
            "Scrapping details from id :loganpaul\n",
            "Scrapping details from id :annakendrick47\n",
            "Scrapping details from id :vamuseum\n",
            "Scrapping details from id :kourtneykardash\n",
            "Scrapping details from id :stylebyand\n",
            "Scrapping details from id :dojacat\n",
            "Scrapping details from id :katstickler\n",
            "Scrapping details from id :caradelevingne\n",
            "Scrapping details from id :vanessahudgens\n",
            "Scrapping details from id :and_techno\n",
            "Scrapping details from id :hichasestokes\n",
            "Scrapping details from id :piperrockelle\n",
            "Scrapping details from id :gigihadid\n",
            "Scrapping details from id :phoebejtonkin\n",
            "Scrapping details from id :addisonraee\n",
            "Scrapping details from id :and.bloom\n",
            "Scrapping details from id :instadanjlevy\n",
            "Scrapping details from id :katehudson\n",
            "Scrapping details from id :frankandoak\n",
            "Scrapping details from id :kateandlauramulleavy\n",
            "Scrapping details from id :vinniehacker\n",
            "Scrapping details from id :bumbleandbumble\n",
            "Scrapping details from id :finnwolfhardofficial\n",
            "Scrapping details from id :mirandakerr\n",
            "Scrapping details from id :artem_odnovol\n",
            "Scrapping details from id :inezandvinoodh\n",
            "Scrapping details from id :amybethmcnulty\n",
            "Scrapping details from id :charlidamelio\n",
            "Scrapping details from id :colesprouse\n",
            "Scrapping details from id :tchalamet\n",
            "Scrapping details from id :officialtomellis\n",
            "Scrapping details from id :ncentineo\n",
            "Scrapping details from id :crateandkids\n",
            "Scrapping details from id :kehlani\n",
            "Scrapping details from id :madonna\n",
            "Scrapping details from id :saweetie\n",
            "Scrapping details from id :willowsmith\n",
            "Scrapping details from id :ofmonstersandmen\n",
            "Scrapping details from id :wwe\n",
            "Scrapping details from id :brooklynandbailey\n",
            "Scrapping details from id :daviddobrik\n",
            "Scrapping details from id :shawnmendes\n",
            "Scrapping details from id :war.and.peas\n",
            "Scrapping details from id :alisha\n",
            "Scrapping details from id :sommerray\n",
            "Scrapping details from id :juleshough\n",
            "Scrapping details from id :bellathorne\n",
            "Scrapping details from id :caratsandcake\n",
            "Scrapping details from id :machandmach\n",
            "Scrapping details from id :sydney_sweeney\n",
            "Scrapping details from id :josephparkmd\n",
            "Scrapping details from id :benaffleck\n",
            "Scrapping details from id :theweeknd\n",
            "Scrapping details from id :nichlmao\n",
            "Scrapping details from id :netflix\n",
            "Scrapping details from id :teganandsara\n",
            "Scrapping details from id :charlizeafrica\n",
            "Scrapping details from id :kevinrsamuels\n",
            "Scrapping details from id :betterhomesandgardens\n",
            "Scrapping details from id :fameandpartners\n",
            "Scrapping details from id :janieandjack\n",
            "Scrapping details from id :cookieandkate\n",
            "Scrapping details from id :tiffanyandco\n",
            "Scrapping details from id :jenniferaniston\n",
            "Scrapping details from id :lorenbrovarnik\n",
            "Scrapping details from id :travisscott\n",
            "Scrapping details from id :itsmarziapie\n",
            "Scrapping details from id :barsandmelody\n",
            "Scrapping details from id :davidmuirabc\n",
            "Scrapping details from id :wigsandtea\n",
            "Scrapping details from id :lizandmollie\n",
            "Scrapping details from id :madelyncline\n",
            "Scrapping details from id :clementstwins\n",
            "Scrapping details from id :youngandfree\n",
            "Scrapping details from id :corinnakopf\n",
            "Scrapping details from id :eatyourkimchi\n",
            "Scrapping details from id :janetguzman\n",
            "Scrapping details from id :theprettymess\n",
            "Scrapping details from id :tomdaley\n",
            "Scrapping details from id :luluandgeorgia\n",
            "Scrapping details from id :selenagomez\n",
            "Scrapping details from id :kevinhart4real\n",
            "Scrapping details from id :evangelinelillyofficial\n",
            "Scrapping details from id :justlikethatcloset\n",
            "Scrapping details from id :stickks__\n",
            "Scrapping details from id :gardenandgun\n",
            "Scrapping details from id :puppetsandpuppets\n",
            "Scrapping details from id :jamescharles\n",
            "Scrapping details from id :tiffanyfrancosmith\n",
            "Scrapping details from id :dirtybootsandmessyhair\n",
            "Scrapping details from id :diddy\n",
            "Scrapping details from id :luigiandiango\n",
            "Scrapping details from id :mapleandmortycorgi\n",
            "Scrapping details from id :cameronreidhamilton\n",
            "Scrapping details from id :loveandsaltla\n",
            "Scrapping details from id :theshaderoomteens\n",
            "Scrapping details from id :niallhoran\n",
            "Scrapping details from id :createcultivate\n",
            "Scrapping details from id :jessiejamesdecker\n",
            "Scrapping details from id :tiffanyfrancosmith\n",
            "Scrapping details from id :war.and.peas\n",
            "Scrapping details from id :selenagomez\n",
            "Scrapping details from id :tenzofficial\n",
            "Scrapping details from id :lorenbrovarnik\n",
            "Scrapping details from id :caratsandcake\n",
            "Scrapping details from id :tankandthebangas\n",
            "Scrapping details from id :emmawatson\n",
            "Scrapping details from id :cocoandbreezy\n",
            "Scrapping details from id :btbamofficial\n",
            "Scrapping details from id :peaslovencarrots\n",
            "Scrapping details from id :hubman.chubgirl\n",
            "Scrapping details from id :changoandco\n",
            "Scrapping details from id :zacefron\n",
            "Scrapping details from id :stoneandstrand\n",
            "Scrapping details from id :kellyripa\n",
            "Scrapping details from id :needleandthreadlondon\n",
            "Scrapping details from id :ploo3o\n",
            "Scrapping details from id :tenzofficial\n",
            "Scrapping details from id :peakandvalleyco\n",
            "Scrapping details from id :ralphthecorgi\n",
            "Scrapping details from id :dovecameron\n",
            "Scrapping details from id :shadowandbone\n",
            "Scrapping details from id :lupitanyongo\n",
            "Scrapping details from id :camerondiaz\n",
            "Scrapping details from id :grimes\n",
            "Scrapping details from id :prideofgypsies\n",
            "Scrapping details from id :brendanmorais\n",
            "Scrapping details from id :twooodley\n",
            "Scrapping details from id :madisonpettis\n",
            "Scrapping details from id :jaredleto\n",
            "Scrapping details from id :daxshepard\n",
            "Scrapping details from id :sweatandtell\n",
            "Scrapping details from id :sofiavergara\n",
            "Scrapping details from id :bretmanrock\n",
            "Scrapping details from id :djkhaled\n",
            "Scrapping details from id :gordongram\n",
            "Scrapping details from id :kimbellasworld\n",
            "Scrapping details from id :gelo\n",
            "Scrapping details from id :adele\n",
            "Scrapping details from id :karaandnate\n",
            "Scrapping details from id :simonandschuster\n",
            "Scrapping details from id :brendasong\n",
            "Scrapping details from id :chloeandchenelle\n",
            "Scrapping details from id :arianagrande\n",
            "Scrapping details from id :theheadandtheheart\n",
            "Scrapping details from id :theellenshow\n",
            "Scrapping details from id :orleansandyorkdeli\n",
            "Scrapping details from id :hayleymorris3\n",
            "Scrapping details from id :aymie91\n",
            "Scrapping details from id :camila_cabello\n",
            "Scrapping details from id :stephen.and.penelope\n",
            "Scrapping details from id :joytaylortalks\n",
            "Scrapping details from id :natalinanoel\n",
            "Scrapping details from id :chloegmoretz\n",
            "Scrapping details from id :catelynnmtv\n",
            "Scrapping details from id :stephencurry30\n",
            "Scrapping details from id :tylerandtodd\n",
            "Scrapping details from id :emmaroberts\n",
            "Scrapping details from id :ananyapanday\n",
            "Scrapping details from id :jockowillink\n",
            "Scrapping details from id :damonandjo\n",
            "Scrapping details from id :work.and.wonder\n",
            "Scrapping details from id :skaijackson\n",
            "Scrapping details from id :jessicabatten_\n",
            "Scrapping details from id :veteranas_and_rucas\n",
            "Scrapping details from id :piersonwodzynski\n",
            "Scrapping details from id :chaelincl\n",
            "Scrapping details from id :nike\n",
            "Scrapping details from id :taliamar\n",
            "Scrapping details from id :natalieportman\n",
            "Scrapping details from id :nike\n",
            "Scrapping details from id :taliamar\n",
            "Scrapping details from id :natalieportman\n",
            "Scrapping details from id :lovebeautyandplanet\n",
            "Scrapping details from id :madmuseum\n",
            "Scrapping details from id :bellapoarch\n",
            "Scrapping details from id :chloexhalle\n",
            "Scrapping details from id :michaelyerger\n",
            "Scrapping details from id :jillandjordan\n",
            "Scrapping details from id :foodnetwork\n",
            "Scrapping details from id :seed.and.sew\n",
            "Scrapping details from id :laurenlondon\n",
            "Scrapping details from id :luismoravids\n",
            "Scrapping details from id :sarah_g130\n",
            "Scrapping details from id :jennytaft\n",
            "Scrapping details from id :mariefeandjakesnow\n",
            "Scrapping details from id :thegritandpolish\n",
            "Scrapping details from id :laurencohan\n",
            "Scrapping details from id :twochicksandahammer\n",
            "Scrapping details from id :rejectedjokes\n",
            "Scrapping details from id :anaismirabelle\n",
            "Scrapping details from id :mylifeaseva\n",
            "Scrapping details from id :benandjerrys\n",
            "Scrapping details from id :quarterjade\n",
            "Scrapping details from id :adeptthebest\n",
            "Scrapping details from id :tommyinnitt\n",
            "Scrapping details from id :margoandme\n",
            "Scrapping details from id :ilizas\n",
            "Scrapping details from id :lilyrose_depp\n",
            "Scrapping details from id :davidlaid\n",
            "Scrapping details from id :tiamowry\n",
            "Scrapping details from id :alexhonnold\n",
            "Scrapping details from id :alexhonnold\n",
            "Scrapping details from id :myfabolouslife\n",
            "Scrapping details from id :sarahmgellar\n",
            "Scrapping details from id :fitzandthetantrums\n",
            "Scrapping details from id :busyphilipps\n",
            "Scrapping details from id :ariasaki\n",
            "Scrapping details from id :jessicaalba\n",
            "Scrapping details from id :kylerichards18\n",
            "Scrapping details from id :borisandhorton\n",
            "Scrapping details from id :julesleblanc\n",
            "Scrapping details from id :hasandpiker\n",
            "Scrapping details from id :plankandpillow\n",
            "Scrapping details from id :usfws\n",
            "Scrapping details from id :andwander_official\n",
            "Scrapping details from id :andwander_official\n",
            "Scrapping details from id :recycleandplay\n",
            "Scrapping details from id :lostandfoundvintage\n",
            "Scrapping details from id :_ph_skincare\n",
            "Scrapping details from id :cap.ironwidow\n",
            "Scrapping details from id :yuckesh\n",
            "Scrapping details from id :lakor_lashes\n",
            "Scrapping details from id :yusufguler__\n",
            "Scrapping details from id :cocoanddash_sale\n",
            "Scrapping details from id :oalia_perruquers\n",
            "Scrapping details from id :indian_folk_art_gallery\n",
            "Scrapping details from id :monstainfinite_official\n",
            "Scrapping details from id :diegvpvpi\n",
            "Scrapping details from id :audiluxcar\n",
            "Scrapping details from id :code_mane22\n",
            "Scrapping details from id :brownu\n",
            "Scrapping details from id :avenueburgershop\n",
            "Scrapping details from id :playboy\n",
            "Scrapping details from id :butecojk\n",
            "Scrapping details from id :oicarodecarvalho\n"
          ]
        }
      ]
    }
  ]
}