{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Insta Data Scrapping using instagrams profile Directory.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.3 32-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "d9dc88e4aa9c05378aa5e64bcd0e11e2b1e5984d8afad505fdf0864ca3363bab"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Test for Python Developer – API Data Extraction role**\r\n",
        "\r\n",
        "1. Uses Youtube to get channel info and other related data"
      ],
      "metadata": {
        "id": "QVk9nVfXuRRE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import os\r\n",
        "import time\r\n",
        "import re\r\n",
        "import pandas as pd  \r\n",
        "import urllib\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "from selenium import webdriver\r\n",
        "from selenium.webdriver.common.keys import Keys\r\n",
        "from selenium.webdriver.support import expected_conditions as EC\r\n",
        "from selenium.webdriver.common.by import By\r\n",
        "from selenium.webdriver.support.wait import WebDriverWait\r\n",
        "\r\n",
        "from webdriver_manager.chrome import ChromeDriverManager\r\n",
        "from selenium.common.exceptions import TimeoutException\r\n",
        "from selenium.common.exceptions import NoSuchElementException\r\n",
        "\r\n",
        "import xlsxwriter\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "DWitN5wmuNEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step 1 :** Scrape channel info from videos avalaible on youtube homepage"
      ],
      "metadata": {
        "id": "kqTDekfN_we4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "PROJECT_DIR = os.getcwd()\r\n",
        "print(PROJECT_DIR)\r\n",
        "DATA_DIR = os.path.join(PROJECT_DIR,'ScrappedData')\r\n",
        "YT_DATA_DIR = os.path.join(DATA_DIR,'YoutubeData')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c:\\Users\\user\\Desktop\\data_scrape_test\n"
          ]
        }
      ],
      "metadata": {
        "id": "MPrxxQmFzUjX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "if not os.path.exists(PROJECT_DIR):\r\n",
        "  os.mkdir(PROJECT_DIR)\r\n",
        "\r\n",
        "if not os.path.exists(DATA_DIR):\r\n",
        "  os.mkdir(DATA_DIR)\r\n",
        "\r\n",
        "if not os.path.exists(YT_DATA_DIR):\r\n",
        "  os.mkdir(YT_DATA_DIR)"
      ],
      "outputs": [],
      "metadata": {
        "id": "qMoGqX7Wz52r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# !pip install webdriver-manager"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "custom_path=r'c:\\Users\\user\\Desktop\\data_scrape_test'\r\n",
        "\r\n",
        "driver = webdriver.Chrome(ChromeDriverManager(path=custom_path).install())\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "====== WebDriver manager ======\n",
            "Current google-chrome version is 93.0.4577\n",
            "Get LATEST driver version for 93.0.4577\n",
            "Driver [c:\\Users\\user\\Desktop\\data_scrape_test\\drivers\\chromedriver\\win32\\93.0.4577.63\\chromedriver.exe] found in cache\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "#xlsx\r\n",
        "# !pip install xlsxwriter==3.0.1 \r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "def get_channel_url_and_id(driver):\r\n",
        "    channel_names = []\r\n",
        "    urls = []\r\n",
        "    channel_name_elements = driver.find_elements_by_class_name(name='yt-formatted-string')\r\n",
        "\r\n",
        "    for el in channel_name_elements:\r\n",
        "        c_name = el.get_property('text')\r\n",
        "        channel_names.append(c_name)\r\n",
        "        url = el.get_property('href')\r\n",
        "        urls.append(url)\r\n",
        "\r\n",
        "    return channel_names,urls\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "def get_channel_info(driver):\r\n",
        "    channel_names = []\r\n",
        "    urls = []\r\n",
        "    channel_name_elements = driver.find_elements_by_class_name(name='yt-formatted-string')\r\n",
        "\r\n",
        "    for el in channel_name_elements:\r\n",
        "        c_name = el.get_property('text')\r\n",
        "        channel_names.append(c_name)\r\n",
        "        url = el.get_property('href')\r\n",
        "        urls.append(url)\r\n",
        "\r\n",
        "    return channel_names,urls\r\n",
        "\r\n",
        "def get_subscriber_count(text):\r\n",
        "    count = text.split(' subscribers')[0]\r\n",
        "    if 'K' in count:\r\n",
        "        subscribers = count.split('K')[0]\r\n",
        "        subscribers = int(float(subscribers)*1000)\r\n",
        "    elif 'M' in count:\r\n",
        "        subscribers = count.split('M')[0]\r\n",
        "        subscribers = int(float(subscribers)*1000000)\r\n",
        "    else:\r\n",
        "        subscribers = int(count)\r\n",
        "    return subscribers\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "yt_url = 'https://youtube.com'\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#gets channel name and url from yt Home page\r\n",
        "\r\n",
        "timeout = 5\r\n",
        "channel_names = []\r\n",
        "channel_urls = []\r\n",
        "driver.get(yt_url)\r\n",
        "\r\n",
        "##scroll page to get new contnet\r\n",
        "\r\n",
        "#scroll down 2 times\r\n",
        "#increase the range to sroll more\r\n",
        "n_scrolls = 40\r\n",
        "for j in range(0, n_scrolls):\r\n",
        "    to_loc = str(j*2000)\r\n",
        "    arg = \"window.scrollTo(0,{});\".format(to_loc)\r\n",
        "    driver.execute_script(arg)\r\n",
        "    time.sleep(5)\r\n",
        "\r\n",
        "try:\r\n",
        "    element_present = EC.presence_of_element_located((By.ID, 'content'))\r\n",
        "    WebDriverWait(driver, timeout).until(element_present)\r\n",
        "    c_names,c_urls = get_channel_url_and_id(driver)\r\n",
        "    channel_names.extend(c_names)\r\n",
        "    channel_urls.extend(c_urls)\r\n",
        "except TimeoutException:\r\n",
        "    print (\"Timed out waiting for page to load\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#save scraped channel info and channel urls as csv\r\n",
        "\r\n",
        "dict = {'channel_name': channel_names, 'channel_url': channel_urls}  \r\n",
        "\r\n",
        "yt_info = pd.DataFrame(dict) \r\n",
        "    \r\n",
        "# saving the dataframe \r\n",
        "yt_info.to_csv(os.path.join(DATA_DIR,'yt_users_data.csv'))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 complete\r\n",
        "saved channel urls and channel names as a csv file"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "#read the saved csv file to get yt channel urls\r\n",
        "yt_info = pd.read_csv(os.path.join(DATA_DIR,'yt_users_data.csv'))\r\n",
        "\r\n",
        "channel_urls = yt_info['channel_url']\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "yt_info.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>channel_name</th>\n",
              "      <th>channel_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Marley Dockery</td>\n",
              "      <td>https://www.youtube.com/user/MrCornholelio</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Oggy Hindi - हिन्दी</td>\n",
              "      <td>https://www.youtube.com/channel/UC9TgvYNUsMBPl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Quantum Tech HD</td>\n",
              "      <td>https://www.youtube.com/c/QuantumTech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>PRANAV YT FF</td>\n",
              "      <td>https://www.youtube.com/channel/UCWxmHsZD62sHU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>HouseMusicHD</td>\n",
              "      <td>https://www.youtube.com/c/FutureHouseHD</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0         channel_name  \\\n",
              "0           0       Marley Dockery   \n",
              "1           1  Oggy Hindi - हिन्दी   \n",
              "2           2      Quantum Tech HD   \n",
              "3           3         PRANAV YT FF   \n",
              "4           4         HouseMusicHD   \n",
              "\n",
              "                                         channel_url  \n",
              "0         https://www.youtube.com/user/MrCornholelio  \n",
              "1  https://www.youtube.com/channel/UC9TgvYNUsMBPl...  \n",
              "2              https://www.youtube.com/c/QuantumTech  \n",
              "3  https://www.youtube.com/channel/UCWxmHsZD62sHU...  \n",
              "4            https://www.youtube.com/c/FutureHouseHD  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "def get_image(url,name):\r\n",
        "   urllib.request.urlretrieve(url, name)\r\n",
        "   i = Image.open(name)\r\n",
        "   i.thumbnail((100, 100), Image.LANCZOS)\r\n",
        "   i.save(name)\r\n",
        "\r\n",
        "def get_videos_info(driver,n=12):\r\n",
        "    timeout = 5\r\n",
        "    video_urls = []\r\n",
        "    video_durations = []\r\n",
        "    video_thumbnails = []\r\n",
        "    video_titles = []\r\n",
        "    channel_urls = yt_info['channel_url']\r\n",
        "    driver.get(channel_urls[0]+\"/videos\")\r\n",
        "    ##scroll page to get new contnet\r\n",
        "    #increase the range to sroll more\r\n",
        "    n_scrolls = 2\r\n",
        "    for j in range(0, n_scrolls):\r\n",
        "        to_loc = str(j*2000)\r\n",
        "        arg = \"window.scrollTo(0,{});\".format(to_loc)\r\n",
        "        driver.execute_script(arg)\r\n",
        "        time.sleep(5)\r\n",
        "    try:\r\n",
        "        element_present = EC.presence_of_element_located((By.ID, 'thumbnail'))\r\n",
        "        WebDriverWait(driver, timeout).until(element_present)\r\n",
        "        video_links = driver.find_elements_by_id('thumbnail')[:n]\r\n",
        "        duration_list = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-thumbnail-overlay-time-status-renderer']\")[:n]\r\n",
        "        thumbnails_list = driver.find_elements_by_id('img')[1:n+1]\r\n",
        "        video_titles = driver.find_elements_by_id('video-title')[:n]\r\n",
        "        views_list = []\r\n",
        "        premiered = []\r\n",
        "        likes = []\r\n",
        "        dislikes = []\r\n",
        "        descriptions = []\r\n",
        "        comments_list = []\r\n",
        "        hashtags_list = []\r\n",
        "\r\n",
        "        ret = {'links':[x.get_attribute('href') for x in video_links],'durations':[x.text for x in duration_list],'thumbnails':[x.get_attribute('src') for x in thumbnails_list],'titles': [x.text for x in video_titles] }\r\n",
        "        for link in ret['links']:\r\n",
        "            try:\r\n",
        "                driver.get(link)\r\n",
        "                element_present = EC.presence_of_element_located((By.ID, 'count'))\r\n",
        "                WebDriverWait(driver, timeout).until(element_present)\r\n",
        "                arg = \"window.scrollTo(0,{});\".format(700)\r\n",
        "                driver.execute_script(arg)\r\n",
        "                time.sleep(4)\r\n",
        "                #get video stats\r\n",
        "                views = driver.find_elements_by_id('count')[1].text.split(' views')[0]\r\n",
        "                views_list.append(views)\r\n",
        "                try:\r\n",
        "                    premere_date = driver.find_elements_by_xpath(\"//yt-formatted-string[@class='style-scope ytd-video-primary-info-renderer']\")[1].text.split('Premiered ')[1]\r\n",
        "                except:\r\n",
        "                    premere_date = 'NA'\r\n",
        "                premiered.append(premere_date)\r\n",
        "                num_likes = driver.find_elements_by_xpath(\"//yt-formatted-string[@class='style-scope ytd-toggle-button-renderer style-text']\")[2].text\r\n",
        "                num_dis_likes = driver.find_elements_by_xpath(\"//yt-formatted-string[@class='style-scope ytd-toggle-button-renderer style-text']\")[3].text\r\n",
        "                likes.append(num_likes)\r\n",
        "                dislikes.append(num_dis_likes)\r\n",
        "                desc = driver.find_elements_by_id('description')[1].text\r\n",
        "                descriptions.append(desc)\r\n",
        "                hashtags = regex = re.findall(\"#(\\w+)\", desc)\r\n",
        "                hashtags = \"#\"+\",#\".join(hashtags)\r\n",
        "                hashtags_list.append(hashtags)\r\n",
        "                try:\r\n",
        "                    num_comments = driver.find_elements_by_id('count')[2].text.split(' Comments')[0]\r\n",
        "                except:\r\n",
        "                    num_comments = \"NA\"\r\n",
        "                comments_list.append(num_comments)\r\n",
        "\r\n",
        "            except TimeoutException:\r\n",
        "                print (\"Timed out waiting for page to load\")\r\n",
        "\r\n",
        "\r\n",
        "    except TimeoutException:\r\n",
        "        print (\"Timed out waiting for page to load\")\r\n",
        "        \r\n",
        "\r\n",
        "    return {'links':ret['links'],'durations':ret['durations'],'thumbnails':ret['thumbnails'],\r\n",
        "    'titles': ret['titles'],'views':views_list,'premiere_date':premiered,'likes':likes,'dislikes':dislikes,'description':descriptions,'hashtags':hashtags_list,'comments':comments_list }"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "def remove_png_images():\r\n",
        "    print(\"Removing Old images\")\r\n",
        "    images = os.listdir(PROJECT_DIR)\r\n",
        "    for image in images:\r\n",
        "        if image.endswith(\".png\"):\r\n",
        "            os.remove(os.path.join(PROJECT_DIR, image))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "#gets channel stats and downloads dp as dp.png of channel\r\n",
        "#cretates a xls file name as channel name\r\n",
        "# and saves the stats obtained\r\n",
        "#and gets stats of few videos of channel\r\n",
        "\r\n",
        "def get_channel_details(channel_url):\r\n",
        "    about_page_url = channel_url+'/about'\r\n",
        "    try:\r\n",
        "        driver.get(about_page_url)\r\n",
        "        element_present = EC.presence_of_element_located((By.ID, 'right-column'))\r\n",
        "        WebDriverWait(driver, timeout).until(element_present)\r\n",
        "        channel_name = driver.find_element_by_class_name('style-scope ytd-channel-name').text\r\n",
        "        \r\n",
        "        try:\r\n",
        "            verified_status = driver.find_element_by_class_name('badge-style-type-verified').get_attribute('aria-label')\r\n",
        "        except NoSuchElementException:\r\n",
        "            verified_status = 'Not-Verified'\r\n",
        "    \r\n",
        "        try:\r\n",
        "            dp_url = driver.find_element_by_id('img').get_attribute('src')\r\n",
        "            get_image(dp_url,'dp.png')\r\n",
        "        except:\r\n",
        "            print(\"DP not found\")\r\n",
        "        #to download image using dp_url\r\n",
        "\r\n",
        "        try:\r\n",
        "            stats = driver.find_element_by_id('right-column').text.split('\\n')\r\n",
        "            joined = stats[1].split('Joined')[1]\r\n",
        "            views = stats[2].split('views')[0]\r\n",
        "\r\n",
        "        except NoSuchElementException:\r\n",
        "            joined =  'NA'\r\n",
        "            views = 'NA'\r\n",
        "\r\n",
        "        description = driver.find_element_by_id('description').text\r\n",
        "        email_ids = re.findall('\\S+@\\S+', description)\r\n",
        "        email_ids_str =  \",\".join(email_ids)\r\n",
        "        soup = BeautifulSoup(driver.page_source,'lxml')\r\n",
        "        links_container = soup.find(id=\"link-list-container\")\r\n",
        "        links_url_list = []\r\n",
        "        links_url = ' '\r\n",
        "        for link in links_container.findChildren('a'):\r\n",
        "            link_url = link.get('href')\r\n",
        "            links_url_list.append(link_url.split('&q=')[1])\r\n",
        "            links_url = ','.join(links_url_list)\r\n",
        "       \r\n",
        "        workbook = xlsxwriter.Workbook(str(YT_DATA_DIR)+'/yt_'+str(channel_name)+'.xlsx')\r\n",
        "        worksheet = workbook.add_worksheet()\r\n",
        "        worksheet.write('A1', 'Name')\r\n",
        "        worksheet.write('B1', 'Url')\r\n",
        "        worksheet.write('C1', 'Subscribers')\r\n",
        "        worksheet.write('D1', 'Verified')\r\n",
        "        worksheet.write('E1', 'description')\r\n",
        "        worksheet.write('F1', 'email')\r\n",
        "        worksheet.write('G1', 'joined')\r\n",
        "        worksheet.write('H1', 'Total views')\r\n",
        "        worksheet.write('I1', 'External Links')\r\n",
        "        worksheet.write('J1', 'Profile Picture url')\r\n",
        "        worksheet.write('K1', 'Profile Picture')\r\n",
        "\r\n",
        "        worksheet.set_row(11,70)\r\n",
        "        worksheet.set_row(1,50)\r\n",
        "        worksheet.set_column_pixels(11,2,80)\r\n",
        "\r\n",
        "        worksheet.write('A2', channel_name)\r\n",
        "        worksheet.write('B2', channel_url)\r\n",
        "        worksheet.write('C2', num_subscribers)\r\n",
        "        worksheet.write('D2', verified_status)\r\n",
        "        worksheet.write('E2', description)\r\n",
        "        worksheet.write('F2', email_ids_str)\r\n",
        "        worksheet.write('G2', joined)\r\n",
        "        worksheet.write('H2', views)\r\n",
        "        worksheet.write('I2', links_url)\r\n",
        "        worksheet.write('J2', dp_url)\r\n",
        "        worksheet.insert_image('K2','dp.png')\r\n",
        "\r\n",
        "        #call another function to save post details in a loop\r\n",
        "        worksheet.set_column('A:A', 30)\r\n",
        "\r\n",
        "    #save recent postsheading\r\n",
        "\r\n",
        "        worksheet.write('A11', 'Thumbnail')\r\n",
        "        worksheet.write('B11', 'Description')\r\n",
        "        worksheet.write('C11', 'Title')\r\n",
        "        worksheet.write('D11', 'Duration')\r\n",
        "        worksheet.write('E11', 'Views')\r\n",
        "        worksheet.write('F11', 'likes')\r\n",
        "        worksheet.write('G11', 'Dislikes')\r\n",
        "        worksheet.write('H11', 'Premiered')\r\n",
        "        worksheet.write('I11', 'Hashtags')\r\n",
        "        worksheet.write('J11', 'Comments')\r\n",
        "        worksheet.write('K11', 'link')\r\n",
        "        posts_info = get_videos_info(driver,6)\r\n",
        "        row = 12\r\n",
        "        for im in posts_info['thumbnails']:\r\n",
        "            if im != None:\r\n",
        "                get_image(im, str(row)+'.png')\r\n",
        "                worksheet.insert_image('A'+str(row), str(row)+'.png')\r\n",
        "            else:\r\n",
        "                worksheet.write_row(0,12,'NA')\r\n",
        "\r\n",
        "            row = row +1\r\n",
        "    \r\n",
        "        worksheet.write_column(12,1,posts_info['description'])\r\n",
        "        worksheet.write_column(12,2,posts_info['titles'])\r\n",
        "        worksheet.write_column(12,3,posts_info['durations'])\r\n",
        "        worksheet.write_column(12,4,posts_info['views'])\r\n",
        "        worksheet.write_column(12,5,posts_info['likes'])\r\n",
        "        worksheet.write_column(12,6,posts_info['dislikes'])\r\n",
        "        worksheet.write_column(12,7,posts_info['premiere_date'])\r\n",
        "        worksheet.write_column(12,8,posts_info['hashtags'])\r\n",
        "        worksheet.write_column(12,9,posts_info['comments'])\r\n",
        "        worksheet.write_column(12,10,posts_info['links'])\r\n",
        "\r\n",
        "\r\n",
        "        workbook.close()\r\n",
        "        #remove all png images\r\n",
        "        remove_png_images()\r\n",
        "        print(\"File Saved as \"+'yt_'+str(channel_name)+'.xlsx')\r\n",
        "    except TimeoutException:\r\n",
        "        print (\"Timed out waiting for page to load\")\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "timeout = 6\r\n",
        "#first checks subscriber count\r\n",
        "for channel_url in channel_urls[:11]:\r\n",
        "    try:\r\n",
        "        driver.get(channel_url)\r\n",
        "        element_present = EC.presence_of_element_located((By.ID, 'thumbnail'))\r\n",
        "        WebDriverWait(driver, timeout).until(element_present)\r\n",
        "        subscriber_count_element = driver.find_element_by_id('subscriber-count')\r\n",
        "        try:\r\n",
        "            num_subscribers = get_subscriber_count(subscriber_count_element.text)\r\n",
        "            if(num_subscribers > 10000):\r\n",
        "                print(\"Subscribers : {} Channel : {}\".format(num_subscribers,channel_url))\r\n",
        "                get_channel_details(channel_url)\r\n",
        "        except:\r\n",
        "            print('Skipping as no subscriber infofound')\r\n",
        "\r\n",
        "    except TimeoutException:\r\n",
        "        print (\"Timed out waiting for page to load\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subscribers : 82200 Channel : https://www.youtube.com/user/MrCornholelio\n",
            "Removing Old images\n",
            "File Saved as yt_Marley Dockery.xlsx\n",
            "Subscribers : 6900000 Channel : https://www.youtube.com/channel/UC9TgvYNUsMBPl6J2xbL-4qg\n",
            "Timed out waiting for page to load\n",
            "Removing Old images\n",
            "File Saved as yt_Oggy Hindi - हिन्दी.xlsx\n",
            "Subscribers : 9620000 Channel : https://www.youtube.com/c/QuantumTech\n",
            "Timed out waiting for page to load\n",
            "Removing Old images\n",
            "File Saved as yt_Quantum Tech HD.xlsx\n",
            "Skipping as no subscriber infofound\n",
            "Subscribers : 6040000 Channel : https://www.youtube.com/c/FutureHouseHD\n",
            "Timed out waiting for page to load\n",
            "Removing Old images\n",
            "File Saved as yt_HouseMusicHD.xlsx\n",
            "Subscribers : 3340000 Channel : https://www.youtube.com/c/Inventor101\n",
            "Removing Old images\n",
            "File Saved as yt_Inventor 101.xlsx\n",
            "Subscribers : 2089999 Channel : https://www.youtube.com/c/WhatTheFukrey\n",
            "Skipping as no subscriber infofound\n",
            "Subscribers : 6210000 Channel : https://www.youtube.com/c/UjjwalGamer\n",
            "Skipping as no subscriber infofound\n",
            "Subscribers : 4530000 Channel : https://www.youtube.com/user/beyounick\n",
            "Timed out waiting for page to load\n",
            "Subscribers : 46400000 Channel : https://www.youtube.com/c/SonyMusicIndia\n",
            "Skipping as no subscriber infofound\n",
            "Subscribers : 281000 Channel : https://www.youtube.com/channel/UC_GYQT7DtaHaRbSeXyWBZyA\n",
            "Removing Old images\n",
            "File Saved as yt_Arun Karthick.xlsx\n"
          ]
        }
      ],
      "metadata": {}
    }
  ]
}